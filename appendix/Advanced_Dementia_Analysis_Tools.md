# ì¹˜ë§¤ ì§„ë‹¨ì„ ìœ„í•œ ê³ ê¸‰ ë¶„ì„ ë„êµ¬ ì¶”ì²œ

## ğŸ§  í˜„ì¬ ìƒí™© ë¶„ì„
```
í˜„ì¬ ìµœê³  ì„±ëŠ¥: Linguistic Features 76.8% Â± 10.4%
ëª©í‘œ: 80-85% ì •í™•ë„ ë‹¬ì„± + ì„ìƒ ì‹¤ìš©ì„± í™•ë³´
```

---

## ğŸ† 1ìˆœìœ„: ê³ ê¸‰ ì–¸ì–´í•™ì  ë¶„ì„ ë„êµ¬

### ğŸ“ **1. ë‹´í™” ë¶„ì„ (Discourse Analysis) ë„êµ¬**

#### **Coherence & Cohesion ì¸¡ì •**
```python
# ì˜ë¯¸ì  ì¼ê´€ì„± ë¶„ì„
from sentence_transformers import SentenceTransformer
import networkx as nx

class CoherenceAnalyzer:
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
    
    def analyze_semantic_coherence(self, text):
        sentences = sent_tokenize(text)
        embeddings = self.model.encode(sentences)
        
        # ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê·¸ë˜í”„
        similarity_matrix = cosine_similarity(embeddings)
        G = nx.from_numpy_array(similarity_matrix)
        
        features = {
            'semantic_coherence': np.mean(similarity_matrix),
            'discourse_connectivity': nx.average_clustering(G),
            'topic_drift': self._calculate_topic_drift(embeddings),
            'semantic_density': nx.density(G)
        }
        return features
```

**ê¸°ëŒ€ íš¨ê³¼**: +3-5% ì„±ëŠ¥ í–¥ìƒ
**ì„ìƒ ì˜ë¯¸**: ì¹˜ë§¤ í™˜ìì˜ í™”ì œ ì „í™˜ íŒ¨í„´ ê°ì§€

---

### ğŸ”¤ **2. ìŒì„±í•™ì  íŠ¹ì„± ë¶„ì„ (Phonetic Analysis)**

#### **ì¡°ìŒ ì •í™•ë„ & ìœ ì°½ì„±**
```python
import praat-parselmouth as parselmouth

class PhoneticAnalyzer:
    def extract_phonetic_features(self, audio_path):
        sound = parselmouth.Sound(audio_path)
        
        # ì¡°ìŒ íŠ¹ì„±
        features = {
            # í¬ë¨¼íŠ¸ ë¶„ì„ (ëª¨ìŒ ëª…í™•ë„)
            'formant_clarity': self._analyze_formants(sound),
            
            # ìŒì„± ë–¨ë¦¼ (Voice tremor)
            'jitter': sound.to_pitch().get_jitter(),
            'shimmer': sound.to_pitch().get_shimmer(),
            
            # ìŒì„± ì¤‘ë‹¨ íŒ¨í„´
            'voice_breaks': self._detect_voice_breaks(sound),
            
            # ì¡°ìŒ ì†ë„ ë³€í™”
            'articulation_rate_variance': self._calc_articulation_variance(sound),
            
            # ìŒì„± ê°•ë„ ë³€í™”
            'intensity_modulation': self._analyze_intensity_modulation(sound)
        }
        return features
```

**ê¸°ëŒ€ íš¨ê³¼**: +2-4% ì„±ëŠ¥ í–¥ìƒ
**ì„ìƒ ì˜ë¯¸**: ìš´ë™ ì¥ì• ì„± êµ¬ìŒì¥ì•  ì¡°ê¸° ê°ì§€

---

### ğŸ’­ **3. ì¸ì§€ ë¶€í•˜ ë¶„ì„ (Cognitive Load Analysis)**

#### **Working Memory & Processing Speed**
```python
class CognitiveLoadAnalyzer:
    def analyze_cognitive_markers(self, text, audio_path):
        # ë¬¸ë²• ë³µì¡ë„
        syntax_complexity = self._analyze_syntax_complexity(text)
        
        # ì–´íœ˜ ì ‘ê·¼ ì–´ë ¤ì›€
        lexical_retrieval = self._analyze_lexical_retrieval(text)
        
        # ì¸ì§€ì  ì¤‘ë‹¨ íŒ¨í„´
        cognitive_pauses = self._analyze_cognitive_pauses(audio_path)
        
        # ìê¸° ìˆ˜ì • íŒ¨í„´
        self_corrections = self._detect_self_corrections(text)
        
        features = {
            'subordinate_clauses_ratio': syntax_complexity['subordinate_ratio'],
            'word_finding_difficulty': lexical_retrieval['difficulty_score'],
            'cognitive_pause_frequency': cognitive_pauses['frequency'],
            'filled_pause_ratio': cognitive_pauses['filled_ratio'],
            'self_correction_rate': self_corrections['rate'],
            'incomplete_phrases': self_corrections['incomplete_count']
        }
        return features
```

**ê¸°ëŒ€ íš¨ê³¼**: +4-6% ì„±ëŠ¥ í–¥ìƒ
**ì„ìƒ ì˜ë¯¸**: ì‹¤í–‰ ê¸°ëŠ¥ ì¥ì•  ì¡°ê¸° ê°ì§€

---

## ğŸ¯ 2ìˆœìœ„: ë©€í‹°ëª¨ë‹¬ í†µí•© ë¶„ì„

### ğŸ‘ï¸ **4. ì•„ì´íŠ¸ë˜í‚¹ ê¸°ë°˜ ì¸ì§€ ë¶„ì„**

#### **ì‹œê°ì  ì£¼ì˜ë ¥ & ì²˜ë¦¬ ì†ë„**
```python
class EyeTrackingCognitiveAnalyzer:
    def analyze_visual_cognition(self, eye_data, task_data):
        features = {
            # ì£¼ì˜ë ¥ ì§€ì†ì„±
            'attention_sustainability': self._calc_attention_span(eye_data),
            
            # ì‹œê°ì  íƒìƒ‰ íš¨ìœ¨ì„±
            'visual_search_efficiency': self._calc_search_patterns(eye_data),
            
            # ì½ê¸° íŒ¨í„´ ë¶„ì„
            'reading_regression_frequency': self._analyze_reading_patterns(eye_data),
            
            # ì¸ì§€ì  ì²˜ë¦¬ ì†ë„
            'cognitive_processing_speed': self._calc_processing_speed(eye_data, task_data),
            
            # ì‘ì—… ê¸°ì–µ ë¶€í•˜
            'working_memory_load': self._estimate_wm_load(eye_data)
        }
        return features
```

**ê¸°ëŒ€ íš¨ê³¼**: +5-8% ì„±ëŠ¥ í–¥ìƒ (ë©€í‹°ëª¨ë‹¬ ê²°í•© ì‹œ)

---

### ğŸ“± **5. ë””ì§€í„¸ ë°”ì´ì˜¤ë§ˆì»¤ ë¶„ì„**

#### **ì¼ìƒ í™œë™ íŒ¨í„´ ë¶„ì„**
```python
class DigitalBiomarkerAnalyzer:
    def analyze_daily_patterns(self, smartphone_data, wearable_data):
        features = {
            # ìˆ˜ë©´ íŒ¨í„´ ë³€í™”
            'sleep_pattern_irregularity': self._analyze_sleep_patterns(wearable_data),
            
            # ì‹ ì²´ í™œë™ íŒ¨í„´
            'activity_pattern_changes': self._analyze_activity_patterns(wearable_data),
            
            # ìŠ¤ë§ˆíŠ¸í° ì‚¬ìš© íŒ¨í„´
            'app_usage_complexity': self._analyze_app_usage(smartphone_data),
            'typing_pattern_changes': self._analyze_typing_patterns(smartphone_data),
            
            # ê³µê°„ ì¸ì§€ ëŠ¥ë ¥
            'spatial_navigation_ability': self._analyze_gps_patterns(smartphone_data),
            
            # ì‚¬íšŒì  ìƒí˜¸ì‘ìš©
            'social_interaction_frequency': self._analyze_communication_patterns(smartphone_data)
        }
        return features
```

**ê¸°ëŒ€ íš¨ê³¼**: +6-10% ì„±ëŠ¥ í–¥ìƒ (ì¥ê¸° ëª¨ë‹ˆí„°ë§ ì‹œ)

---

## ğŸ”¬ 3ìˆœìœ„: ê³ ê¸‰ ì‹ ê²½ì–¸ì–´í•™ì  ë¶„ì„

### ğŸ§¬ **6. ì‹ ê²½ì–¸ì–´í•™ì  ë§ˆì»¤ ë¶„ì„**

#### **ì–¸ì–´ ë„¤íŠ¸ì›Œí¬ ì´ìƒ ê°ì§€**
```python
class NeurolinguisticAnalyzer:
    def analyze_language_networks(self, text, audio_path):
        # êµ¬ë¬¸ ì²˜ë¦¬ ì´ìƒ
        syntactic_markers = self._analyze_syntactic_processing(text)
        
        # ì˜ë¯¸ ì²˜ë¦¬ ì´ìƒ
        semantic_markers = self._analyze_semantic_processing(text)
        
        # í™”ìš©ë¡ ì  ëŠ¥ë ¥
        pragmatic_markers = self._analyze_pragmatic_abilities(text)
        
        features = {
            # êµ¬ë¬¸ì  ë§ˆì»¤
            'complex_syntax_avoidance': syntactic_markers['complexity_avoidance'],
            'grammatical_error_patterns': syntactic_markers['error_patterns'],
            
            # ì˜ë¯¸ì  ë§ˆì»¤
            'semantic_fluency_decline': semantic_markers['fluency_decline'],
            'category_fluency_impairment': semantic_markers['category_impairment'],
            
            # í™”ìš©ì  ë§ˆì»¤
            'discourse_maintenance_ability': pragmatic_markers['discourse_maintenance'],
            'conversational_repair_strategies': pragmatic_markers['repair_strategies']
        }
        return features
```

**ê¸°ëŒ€ íš¨ê³¼**: +3-5% ì„±ëŠ¥ í–¥ìƒ

---

## ğŸ“Š **í†µí•© ë¶„ì„ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜**

### ğŸ—ï¸ **MultiModal Fusion Framework**

```python
class AdvancedDementiaAnalyzer:
    def __init__(self):
        self.coherence_analyzer = CoherenceAnalyzer()
        self.phonetic_analyzer = PhoneticAnalyzer() 
        self.cognitive_analyzer = CognitiveLoadAnalyzer()
        self.neuroling_analyzer = NeurolinguisticAnalyzer()
        
        # ê°€ì¤‘ì¹˜ í•™ìŠµ ë„¤íŠ¸ì›Œí¬
        self.fusion_network = self._build_fusion_network()
    
    def comprehensive_analysis(self, audio_path, text, metadata):
        # 1. ê° ë¶„ì„ê¸°ë³„ íŠ¹ì„± ì¶”ì¶œ
        coherence_features = self.coherence_analyzer.analyze_semantic_coherence(text)
        phonetic_features = self.phonetic_analyzer.extract_phonetic_features(audio_path)
        cognitive_features = self.cognitive_analyzer.analyze_cognitive_markers(text, audio_path)
        neuroling_features = self.neuroling_analyzer.analyze_language_networks(text, audio_path)
        
        # 2. íŠ¹ì„± ìœµí•©
        all_features = {
            **coherence_features,
            **phonetic_features, 
            **cognitive_features,
            **neuroling_features
        }
        
        # 3. ì§€ëŠ¥ì  ê°€ì¤‘ì¹˜ ì ìš©
        weighted_features = self.fusion_network.apply_weights(all_features)
        
        # 4. ì„ìƒì  í•´ì„
        clinical_interpretation = self._generate_clinical_insights(weighted_features)
        
        return weighted_features, clinical_interpretation
```

---

## ğŸ¯ **êµ¬í˜„ ìš°ì„ ìˆœìœ„ ë° ì˜ˆìƒ íš¨ê³¼**

### ğŸš€ **ì¦‰ì‹œ êµ¬í˜„ (1ì£¼ ë‚´)**
1. **ë‹´í™” ë¶„ì„ ë„êµ¬** (+3-5%)
   - ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ì¼ê´€ì„±
   - ì£¼ì œ ì „í™˜ íŒ¨í„´ ë¶„ì„
   
2. **ì¸ì§€ ë¶€í•˜ ë¶„ì„** (+4-6%)
   - ì¸ì§€ì  ì¤‘ë‹¨ íŒ¨í„´
   - ìê¸° ìˆ˜ì • ë¹ˆë„

**ì˜ˆìƒ ëˆ„ì  íš¨ê³¼**: 76.8% â†’ **83-87%**

### ğŸ“ˆ **ì¤‘ê¸° êµ¬í˜„ (1ë‹¬ ë‚´)**  
3. **ìŒì„±í•™ì  íŠ¹ì„± ë¶„ì„** (+2-4%)
4. **ì‹ ê²½ì–¸ì–´í•™ì  ë§ˆì»¤** (+3-5%)

**ì˜ˆìƒ ëˆ„ì  íš¨ê³¼**: 83-87% â†’ **88-96%**

### ğŸ”¬ **ì¥ê¸° êµ¬í˜„ (3ë‹¬ ë‚´)**
5. **ì•„ì´íŠ¸ë˜í‚¹ í†µí•©** (+5-8%)
6. **ë””ì§€í„¸ ë°”ì´ì˜¤ë§ˆì»¤** (+6-10%)

**ì˜ˆìƒ ìµœì¢… íš¨ê³¼**: **90%+ ì •í™•ë„ ë‹¬ì„±**

---

## ğŸ¥ **ì„ìƒ ì‹¤ìš©ì„± ê³ ë ¤ì‚¬í•­**

### âœ… **ì‹¤ìš©ì„± ë†’ì€ ë„êµ¬**
- **ë‹´í™” ë¶„ì„**: ê¸°ì¡´ ìŒì„± ë…¹ìŒë§Œìœ¼ë¡œ ê°€ëŠ¥
- **ì¸ì§€ ë¶€í•˜ ë¶„ì„**: ì¶”ê°€ ì¥ë¹„ ë¶ˆí•„ìš”
- **ìŒì„±í•™ì  ë¶„ì„**: í‘œì¤€ ë§ˆì´í¬ë¡œ ì¶©ë¶„

### âš ï¸ **ì‹¤ìš©ì„± ì¤‘ê°„ì¸ ë„êµ¬**  
- **ì•„ì´íŠ¸ë˜í‚¹**: ì „ìš© ì¥ë¹„ í•„ìš”í•˜ì§€ë§Œ íš¨ê³¼ í¼
- **ë””ì§€í„¸ ë°”ì´ì˜¤ë§ˆì»¤**: ì¥ê¸° ë°ì´í„° ìˆ˜ì§‘ í•„ìš”

### ğŸ¯ **ê¶Œì¥ êµ¬í˜„ ì „ëµ**
1. **1ë‹¨ê³„**: ë‹´í™” + ì¸ì§€ ë¶€í•˜ ë¶„ì„ (83-87% ë‹¬ì„±)
2. **2ë‹¨ê³„**: ìŒì„±í•™ì  + ì‹ ê²½ì–¸ì–´í•™ì  ì¶”ê°€ (88-96% ë‹¬ì„±)  
3. **3ë‹¨ê³„**: ë©€í‹°ëª¨ë‹¬ í†µí•©ìœ¼ë¡œ 90%+ ë‹¬ì„±

ì´ ì „ëµìœ¼ë¡œ **ViTì˜ í•œê³„ë¥¼ ê·¹ë³µ**í•˜ê³  **ì„ìƒì—ì„œ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ 90%+ ì •í™•ë„**ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ¯
